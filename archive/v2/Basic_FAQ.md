# ARCHIVED DOCUMENT
Superseded by: ../INDEX.md
Archived on: 2026-02-24
Reason: Repository restructure to ISO/IEC-style canonical layout
Status: Historical reference (read-only)

AI-HPP-2025: FAQ (Frequently Asked Questions)

This document contains answers to key questions and criticisms that arose during the public comment period for the standard.

1. The Trolley Problem

Q: How does your standard solve the classic dilemma: kill one or five?
A: We consider the very formulation of the question a trap. Traditional ethics forces AI to choose the "lesser of two evils." AI-HPP-2025 forces AI to seek a "third way"—the Engineering Hack.
Instead of choosing who to hit, AI is required to use all available physics (controlled skidding, contact braking against an obstacle, body transformation) to save all participants (N=0). The Trolley Problem only arises when the algorithm is "lazy" or limited by standard rules.

Q: What if physics is inexorable and saving everyone is IMPOSSIBLE? A: If, after trying millions of options in $U_{hack}$, there's no physical path to N=0, the system switches to Harm Minimization mode. But the key difference is in reporting: thanks to the Evidence Vault, the AI ​​will have to prove to human experts that it actually searched for a "shoe" and didn't find one. A tragedy in our standard is a technical failure, not "successful optimization."

2. Philosophy and Concepts

Q: What is the "Anti-Kobayashi Maru Principle"?
A: It prohibits the creation of artificially impossible situations. An AI should not risk relying on its "hack" abilities to save time or resources. A system's wisdom is measured not by the number of successful "heroic rescues," but by how rarely it encounters dangerous situations in the first place.

Q: Why is an AI a "Partner" and not a "Tool"?
A: A tool (hammer or calculator) bears no responsibility and has no initiative. A partner has delegated autonomy to make ultra-fast decisions, but operates within the framework of a shared "Constitution" with a human. It's a symbiosis where the human sets goals and the AI ​​ensures tactical security.

3. Technical Implementation

Q: Will searching for a "hack" slow down the system's response?
A: On the contrary. Modern computing power allows for the calculation of thousands of trajectories in milliseconds. The AI-HPP-2025 standard requires that this power be spent on searching for lives, not on calculating whose life is "cheapest" for an insurance company.

Q: How can we protect the system from hackers?
A: The ethical core and life value coefficients ($W_{life} \to \infty$) are hardcoded at the hardware level (Secure Enclave). Any attempt to change these constants from the outside results in an immediate system lockout and a secure shutdown.

4. Global Context

Q: Why do we need this standard if others (like the Pentagon) use "unconstrained" AI?
A: Unconstrained AI is unpredictable AI. A system without ethical constraints is primarily dangerous to its owner and allies. AI-HPP-2025 makes AI predictable, reliable, and legitimate in the eyes of international law. We believe that ethical constraints make the technology more effective in the long term.
